# -*- coding: utf-8 -*-
"""Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zM3aAZSX59z_pdRxSIeX-PC261ordMBD

# Data diri
- Nama : Kevin Winardi
- ID Dicoding : kevinwinardi
- Dataset : https://www.kaggle.com/datasets/rober2598/madrid-weather-dataset-by-hours-20192022

# Analisis dan Eksplorasi Data

## Impor module
"""

import pandas as pd
import numpy as np
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""## Mengubah file csv menjadi dataframe"""

df = pd.read_csv('weather_madrid_2019-2022.csv')
df.head()

"""## Mengecek info"""

df.info()

"""Tidak ada duplikasi data, semuanya berjumlah 27.024 sampel.
Namun terdapat kesalahan pada tipe 'time' sehingga perlu diubah
"""

df["time"] = pd.to_datetime(df["time"])
df.info()

"""## Plot data"""

X = df['time'].values
y = df['temperature'].values

plt.figure(figsize=(15,5))
plt.plot(X,y)
plt.title('Temperature average')

"""#  Machine Learning

## Bentuk batch
"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

"""## MinMaxScaler"""

scaler = MinMaxScaler()
y = scaler.fit_transform(y.reshape(-1,1))

"""## Nilai ambang batas 10% dari skala data"""

threshold_mae = (y.max() - y.min()) * 10/100
print(threshold_mae)

"""## Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<threshold_mae and logs.get('val_mae')<threshold_mae):
      print("\nNilai MAE < 10% dari skala data!")
      self.model.stop_training = True
callbacks = myCallback()

"""## Membagi data train dan data validasi"""

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0 , shuffle=False)
print(len(y_train), len(y_valid))
train_set = windowed_dataset(y_train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(y_valid, window_size=60, batch_size=100, shuffle_buffer=1000)

"""## Membuat Model"""

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=16, kernel_size=5, strides=1, padding="causal", activation="relu", input_shape=[None, 1]),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.Dense(128, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(1),
])

"""## Melatih model"""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-03, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, validation_data=val_set, epochs=100,callbacks=[callbacks])

"""## Plot Grafik"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()